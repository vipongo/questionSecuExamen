L’évaluation se tiendra à distance suivant les modalités suivantes:

* Il s’agira d’un examen en ligne, piloté au travers de la plateforme WebCampus. Il vous faudra donc être connecté(e) sur la plateforme durant tout l’examen et avoir réussi l’examen blanc proposé par la cellule TICE (les détails concernant cet examen blanc suivront)
* La date de l'examen sera confirmée sous peu par le secrétariat facultaire
* Chacun(e) devra être connecté(e) sur Teams durant la durée de l’examen; cela permettra le cas échéant pour vous, de poser des questions, et pour moi, de m’assurer que tout se déroule correctement
* L’examen sera un mélange de questions fermées (QCM, appariement…) et de questions ouvertes. Il s’agit de tester à la fois vos connaissances et vos compétences ainsi que votre capacité de raisonnement. 
* En particulier, vous serez confronté(e)s à une analyse de cas similaire à celle que je vous ai proposée dans le chapitre sur l’analyse de risques
* L’examen se fera à livre ouvert
* De manière générale, l’épreuve consistera à mobiliser les outils et approches vus au cours pour répondre à une situation concrète. Vous pouvez donc vous attendre à devoir effectuer une analyse sommaire des risques sur base d’un cas, et de proposer des solutions appropriées, ces solutions relevant potentiellement des différents chapitres du cours. Les attentes chapitre par chapitre sont détaillées à la fin de ce message.

Comme communiqué au début du cours, l’examen interviendra pour 60% dans la note finale, et les travaux pratiques que vous avez réalisés compteront pour 40%.Je suis conscient que ce mode d’évaluation risque d’être un peu déroutant. Je vous rassure, l’examen ne sera pas plus compliqué que lors des sessions précédentes. Si vous avez la moindre question ou que vous anticipez la moindre difficulté, n’hésitez à m’en faire part, nous trouverons des solutions. En attendant, prenez soin de vous, je reste joignable par les moyens habituels (Teams, email)Jean-Noël Colin 

**Attentes détaillées chapitre par chapitre**

## Chapitre 1 et 2: Introduction et Méthodologie.

### Connaitre les critères de sécurité.

* **Conﬁdentialité**: Information non accessibles aux individus/entités non autorisés, et ce dans le stockage, l'échange ou le traitement.
* **Intégrité:** Propriété de la précision et de l'accomplissement. Par exemple: dans le stockage, l'échange et le traitement.
* **Disponibilité:** Fait d'être accessible et utilisable à la demande par une entité autorisée.
* **Authenticité:** Vériﬁcation qu'une entité est bien celle qu'elle prétend être.
* **Responsabilité:** Les actions peuvent être tracées jusqu'aux entités d'origines.
* **Non-répudiation:** Capacité de prouver qu'une action a bien été perpétrée par une entité.
* **Fiabilité:** Propriété d'un comportement consistant (resultat cohérant et attendu).

### Connaitre les concepts et le déroulement d’une analyse de risques.

**Deux approche:**  
* **Basé sur le risque:** Comprendre, identifié, evaluer, décider, planifier, prioriser, réagir, communique, **ETABLIR ET MAINTENIR UNE CONNAISSANCE DE BASE**   
* **Basé sur la structure:** Définir un vocabulaire commun, une structure claire et procéder, aide à faire une analyse le plus exhaustive possible, assurer la continuité d'une application, améliorer le processus lui-même   
**Qu'est ce qu'un processus?**  
* **Itératif:** Ré-évaluer le système, l’environnement, les choix précédant périodiquement  
* **Récursif:** Evaluer le système, puis ses composant, etc
* **Supporté:** Par beaucoup de méthodes : différents niveaux de complexités et de contexts
Nombreuses méthodes déjà existantes (Octave, CRAMM, EBIOS, Mehari)

### Déroulement: 

**1.Établir le contexte:** Définir les objectifs et le périmètre du processus, pour s’assurer que tous les éléments importants soient pris en compte et que les risques qui pourraient survenir sont correctement identifiés. C’est aussi définir une approche de gestion des risques et des critères basiques:  
* *Critères d’évaluation des risques:* basés sur la valeur des atouts, les obligations légales, et l’importance opérationnelle et business de la disponibilité et de l’intégrité.  
* *Critères d’évaluation de l’impact:* pour exprimer et mesurer le degré de dégâts ou coûts que chaque risque peut provoquer. Il y a plusieurs dimensions : continuité de l’entreprise, dégâts sur la réputation, non respect d’obligations légales ou contractuelles, etc.  
* *critères d’acceptation des risques:* pour identifier le niveau acceptable de risque. Il faut également bien s’organiser pour cette gestion des risques : définir les rôles et responsabilités, définir des procédures de communication, etc.

**2.Identification des risques:** l’objectif est de comprendre ce qui pourrait causer une perte potentielle (d’atouts), et où et pourquoi cette perte pourrait survenir. Il faut donc identifier : 
* *Les atouts:* atouts primaires (processus, informations, connaissances), secondaires (infrastructure, logiciels, bâtiments, personnel), savoir à qui appartient quel atout.
* *Les menaces:* à partir de listes existantes de menaces, de rapports d’incidents, d’avis d’experts, etc. 
* *Les contrôles:* (de sécurité) déjà existants 
* *Les vulnérabilités:* à différents niveaux : environnement, personnel, procédures, infrastructures, logiciels, parties tierces… 
* *Les impacts:* financiers, de réputation, perte de temps…   


**3.Analyse des risques:** on peut avoir un approche qualitative (ex : risque faible, moyen, fort, ou utiliser une échelle de 1 à 10 : c’est facile à comprendre mais subjectif) ou quantitative (valeurs numériques dérivées de modèles, mais c’est difficile d’avoir des données précises). Il faut également évaluer les conséquences ainsi que la probabilité de chaque risque.  

**4.Évaluation des risques:** à partir des critères d’acceptation des risques et de la liste des risques, il faut identifier les risques qu’il faut mitiger et prioriser leur traitement.  

**5.Traitement des risques:** on peut modifier les risques, les éviter (pas souvent possible), les partager (le donner à quelqu’un d’autre, ex : prendre une assurance pour éviter de devoir trop payer en cas d’accident de voiture). Le traitement se fait en fonction du coût de traitement de chacun des risques et du bénéfice que ce traitement peut apporter.  

**6.Acceptation des risques** approbation formelle (par le management) du plan d’action et des risques résiduels.  

**7.Communication des risques:** échanger les informations concernant les risques (existence, nature, importance, traitement…) avec le management et les parties prenantes.  

**8.Monitoring/surveillance des risques:** surveiller les risques et leurs facteurs pour identifier des changements dans le contexte de l’entreprise et pouvoir donner une réponse à ces changements.  



### Les moyens de mesurer le risque et ses composantes, les options de traitement du risques, les lignes de défense (prévention, détection, récupération)



* **Pouvoir effectuer une analyse de risques sommaires et proposer un plan d’action concret, approprié, réaliste sur base de l’analyse réalisée. Ce plan d’action doit se décliner selon les différentes lignes de défense**  
* **Pouvoir produire un arbre d’attaque crédible**  
Technique proposée par Bruce Schneier, consiste à modéliser les attaques possibles et les conditions d’occurrence, en décomposant les objectifs d’attaques en sous-objectifs, récursivement. Exemple : On peut assigner à chaque nœud une valeur possible (P) ou impossible (I), une valeur donnant la difficulté d’un vecteur d’attaque, son coût, etc. Cela permet de voir quel est le coût d’une attaque, quelle est l’attaque la plus probable, et donc ce qu’il faut protéger en premier. 


![](arbredattaque.png)


## Chapitre 3 :Cryptographie
  #### Connaitre les différentes primitives cryptographiques: chiffrement, empreinte numérique, signature numérique et certificat numérique, les objectifs de sécurité liés, les contraintes de mise en oeuvre...  

  *Cryptographie:* science de la création d’algorithmes (ciphers) de (dé)chiffrement.   
  *Cryptanalyse:* science consistant à casser des algorithmes de chiffrement.   
  *Cryptologie:* étude des algorithmes (création + les casser) : cryptographie + cryptanalyse

  **Chiffrement de César:**  
  Décaler les lettres du message original (ex : A devient D, B devient E, etc). On peut généraliser cette technique en substituant chaque lettre par une autre (ex : A devient D, B devient A, etc). Mais ces deux techniques sont faciles à casser, par exemple en analysant la fréquence de chaque lettre (en français, la lettre E apparaît plus souvent, et donc la lettre qui remplace la lettre E sera probablement celle qui apparaît le plus souvent dans le message codé).

  **Chiffrement de Vigenère:**  
  Substitution polyalphabétique : une même lettre du message clair peut, suivant sa position dans le message, être remplacée par des lettres différentes (contrairement au chiffrement de César par exemple). La i-ème lettre du message chiffré est égale à la i-ème lettre du message en clair avec un décalage de : la i-ème lettre de la clé répétée ( ex : keykeykeyke…) modulo 26.

  **Pourquoi ne pas uniquement utiliser un algorithme (au lieu d’utilisé une clé secrète en plus)?**
  C’est plus difficile de garder un algorithme secret, une fois qu’il est connu il devient inutilisable, il faut faire confiance au créateur de l’algorithme. Alors qu’il est plus facile de garder une clé secrète, que si l’algorithme est public, il peut être validé par de nombreux experts, on peut même utiliser plusieurs clés.

  * *Principe de Kerckhoff* : un système cryptographique doit être sécurisé même si l’ensemble du système est connu, mis à part la clé privée.

  **Différente classes d'attaques**  
  Attaques passives (espionnage, pas d’interaction entre l’attaquant et les parties qui communiquent, attaque difficile à détecter), actives (usurpation d’identité (id spoofing), modification de la communication (ajout, modification ou suppression de paquets)… Ex : man in the middle), attaque par rejeu (renvoyé un message émit précédemment), attaque par canal auxiliaire (attaque basée sur de l’information provenant de l’implémentation physique du mécanisme cryptographique, plutôt que via sa cryptanalyse. Ex : se baser sur la consommation énergétique ou les émissions électromagnétiques de l’algorithme), attaque liée au timing (déterminer des informations sur le processus cryptographique sur base du temps qu’il prend).  

  *On peut attaquer la clé:* à voir en fonction de la taille de la clé (40 bits → 10¹², 56 bits → 64*10¹⁵, mais il y a des clés de 128, 256, 4096 bits!), on peut faire du brute force (en moyenne, on essaie la moitié des possibilités → ajouter un bit à la clé double le temps nécessaire pour un brute force), ou une attaque par dictionnaire (se baser sur une liste de valeurs probables).  

  *On peut attaquer l’algorithme:* l’objectif est alors de trouver la clé plus rapidement que via un brute force. Différents types d’attaques, en fonction de ce que l’on sait connaître : soit on a uniquement accès à un texte chiffré, soit on a accès à des paires de textes chiffrés et déchiffrés, soit on peut obtenir le texte chiffré correspondant à n’importe quel texte déchiffré, soit on peut connaître le texte déchiffré correspondant à n’importe quel texte chiffré (en connaissant la clé).

  **Un bon algorithme de chiffrement** ne doit pas permettre à un attaquant de trouver la clé, ne doit pas permettre à un attaquant de récupérer tout ou une partie du texte en clair, ni aucune propriété du texte en clair (ex : sa longueur précise). Formellement : la probabilité de trouver le texte en clair en connaissant le texte chiffré doit être égale à la probabilité de trouver le texte en clair sans connaître ce texte chiffré.  

  *Les nombres aléatoires* sont la base de nombreux mécanismes de cryptographie : génération de clés, de nonces (nonce : nombre arbitraire destiné à être utilisé une seule fois)… Il faut donc disposer d’un générateur de nombres pseudo-aléatoires (PRNG), se basant sur une graine (seed) réellement aléatoire. Pour obtenir cette graine, on peut se baser sur des phénomènes matériels (bruit thermique des résistances, turbulences d’air dans le disque dur…) ou sur du logiciel (horloge, mouvements de la souris, paramètres du système d’exploitation…). Un générateur de nombres pseudo-aléatoires se base sur un algorithme déterministe, il génère une séquence de nombres qui ont l’air aléatoires sur base d’une graine réellement aléatoire.

  ### Cryptographie symétrique  
  **ECB** – Electronic CodeBook Mode : simplement les juxtaposer. En faisant cela, les blocs identiques seront chiffrés de la même manière et on les remarque facilement.  
  **CBC** – Cipher Block Chaining : pour chaque bloc à chiffrer, on fait un XOR avec le texte chiffré du bloc précédent avant de réaliser le chiffrement du bloc (et pour le premier bloc on fait le XOR avec une constante que l’on transmet en clair avec le message).  
  **CFB** – Cipher FeedBack Mode : pour chaque bloc à chiffrer, on fait un XOR avec Ek(bloc chiffré précédent), et on obtient directement le bloc chiffré.  
  **CTR** – Counter Mode : pour chiffrer un bloc, on fait un XOR avec Ek(nonce || x), où x est le numéro du bloc à chiffrer, et nonce un nombre aléatoire.
  * Cette technique permet la parallélisation (on peut chiffrer plusieurs blocs en même temps car un bloc ne dépend pas du précédent) et permet de déchiffrer seulement une partie du message (car les blocs sont indépendants).  

  **Padding** : si la taille des données n’est pas un multiple de la taille d’un bloc, comment compléter les données pour que le destinataire sache à partir de quand ce ne sont plus des données dans le message en clair : on peut par exemple compter le nombre de bytes qu’il faut rajouter, et écrire cette valeur de manière répétée sur ces bytes à compléter. Le destinataire n’a qu’à lire le dernier byte, il obtient le nombre de bytes qu’il doit supprimer du message. 

  ### Methodes de chiffrement:

  * **Méthode de chiffrement – bloc à usage unique:**  
    Cette méthode est basée sur l’utilisation unique d’un flux aléatoire de données (k) faisant la même taille que les données à chiffrer. Chiffrement : c = p XOR k, et déchiffrement : p = c XOR k. Malheureusement la gestion des clés est alors complexe (il faut changer la clé à chaque fois et elles sont grandes), cette méthode n’est donc pas utilisable en pratique  
  
  * **Méthode de chiffrement – chiffrement de flux:**
    Cette méthode est inspirée de la précédente (bloc à usage unique), on a un générateur de nombres aléatoires qui génère une séquence de bits, et le chiffrement/déchiffrement fonctionne un bit à la fois : c = p XOR pad, p = c XOR pad. Il faut que le générateur soit cryptographiquement sécurisé et correctement initialisé.  
  
  * **Méthode de chiffrement – DES – Digital Encryption Standard:**
    utilise une clé de 56 bits, un chiffrement par blocs de 64 bits, utilise la même fonction pour chiffrer et déchiffrer. L’objectif de cette méthode est que chaque bit du texte chiffré dépende de chaque bit du texte clair et de la clé, et soit aussi rapide que possible. L’algorithme effectue 16 tours (round), en utilisant des clés dérivées de la clé principale (via des permutations et compressions). L’algorithme est plutôt lent, et n’est plus sécurisé (crackable très rapidement).  
  
    * *Triple DES:* Effectuer le chiffrement DES 3 fois, avec 3 clés différentes (donc 168 bits au total). C’est donc 3 fois plus lent que DES.
    * *AES – Advanced Encryption Standard:*: Chiffrement par blocs de 128 bits, avec des clés de 128, 192 ou 256 bits.
    * *RC4:* Méthode de chiffrement utilisé pour la sécurité WiFi « WEP », était censée être secrète (mais s’est retrouvée sur internet), la clé est de longueur variable, et cette méthode génère des séquences aléatoires sur base de la clé, que l’on XOR avec le texte pour le chiffrer. Cette méthode utilise une boite de substitution, qui est mise à jour après chaque utilisation.

  ### Gestion des clés:
  La gestion des clés comprend la gestion des créations, expirations, révocations, envoi et stockage des clés, ainsi que la gestion des menaces : perte, vol, compromission, extorsion… Il peut potentiellement y avoir beaucoup de clés : par exemple si on a n personnes, il faut n*(n-1)/2 clés pour que chacune puisse communiquer individuellement avec toutes les autres, et si on veut pouvoir avoir des communications de groupe, il en faudrait encore plus. Et c’est plus facile pour un attaquant de voler une clé que de casser un chiffrement.  

  **Durée de vie d’une clé:** Plus elle est utilisée, plus elle a de chance d’être compromise, plus l’impact d’une compromission sera grand (car la clé aura servi à encoder beaucoup de messages), plus elle sera intéressante pour des attaquants et plus la cryptanalyse sera facile. Il faut donc bien définir la durée de vie de chaque clé, et il faut trouver un compromis entre le coût de gestion des clés et l’impact en cas de compromission.  

  **Génération des clés:** Il faut une manière sécurisée de les générer (ne pas avoir un espace restreint par exemple), elles doivent être aléatoires, suffisamment longues et doivent être régénérée périodiquement.

  **Mise à jour des clés:** On peut par exemple calculer la clé suivante à partir de la clé actuelle (kt+1 = f(kt)) en utilisant une fonction non inversible, mais la sécurité de la nouvelle clé dépend directement de la sécurité de la précédente  

  **Destruction des clés:** Sans destruction, il sera toujours possible de l’utiliser. Il faut donc une destruction sécurisée (partout où elle a été stockée : documents, fichiers, mémoire de l’ordinateur…).  

  **Stockage des clés:** Il faut les garder en sécurité des attaquants, par exemple sur un média que l’on peut déconnecter (clé usb, baque, carte de banque).  

  **Échange de clés:** On peut utiliser une autre clé (kek – Key-Encryption Key) pour chiffrer les clés que l’on souhaite échanger (dk – Data Keys), mais il faut alors distribuer manuellement les kek avec un haut niveau de sécurité. On peut aussi découper la clé et envoyer chaque partie d’une manière différente (en espérant que l’attaquant n’a pas accès à tous les canaux utilisés). On peut enfin utiliser un centre de distribution de clés (KDC – Key Distribution Center), qui sera alors un tiers de confiance : ce tiers connaîtra toutes les clés, et est un spoc (single point of failure).

  **Partage de clés avec des tiers:** On peut simplement donner une copie de la clé à un tiers de confiance. On peut aussi découper la clé en différentes parties et donner les parties à des personnes différentes. On peut aussi utiliser un appareil de récupération de la clé, que l’on peut transmettre en cas d’absence.  

  **Propriétaire/détenteur des clés:** Il faut pouvoir être sûr qu’une clé donnée appartient bien à la personne que l’on souhaite contacter : soit en échangeant la clé en face à face, en utilisant un canal déjà sécurisé avec la personne, ou en utilisant un tiers de confiance.  

  **Compromission des clés:** Comment détecter une compromission ? On ne le détecte généralement pas tout de suite. Il faut utiliser un maximum de clés différents pour minimiser l’impact d’un vol de clé.

  ### Cryptographie asymétrique  
  Cryptographie asymétrique (=cryptographie à clé publique) : la clé de chiffrement est différente de la clé de déchiffrement : c = E(k1, p) et p = D(k2, c). k1 et k2 sont différents mais sont mathématiquement liées. Ce qui est chiffré avec k1 ne peut être déchiffré qu’avec k2 et vice-versa. Le principe est de rendre une de ces 2 clés publique (K+ ) et de garder l’autre secrète (K- ).  

  **Principes de base:**  
  on se base sur une fonction f à sens unique : c’est facile de calculer y = f(x) mais difficile de calculer x = f-1(y), sauf si on connaît « l’indice » (matérialisé par K- ). La fonction est difficile à inverser et il est difficile de dériver K- à partir de K+ . C’est basé sur la théorie des nombres complexes : factorisation de grands nombres, logarithmes discrets, courbes elliptiques… C’est plus pratique que la cryptographie symétrique pour la gestion des clés (on peut donner la clé publique à tout le monde, et la protection de la clé privée est de la seule responsabilité du propriétaire de la clé), mais c’est plus lent de chiffrer/déchiffrer un message.

  **Algorithme RSA:**  
  inventé par R. Rivest, A. Shamir et L. Adleman en 1978, cette technique se base sur la factorisation de grands nombres. Génération d’une paire de clés :
  1. choisir deux grands nombres premier aléatoires p et q, et calculer n = p*q 
2. calculer φ(n) = (p-1)(q-1) 
3. choisir un exposant e tel que e < n et que e et φ(n) sont premiers entre eux 
4. calculer d tel que d*e = 1 + k* φ(n) (où k appartient aux naturels) 
5. K + = (n, e) et K- = (n, d).  

    → Généralement, p et q font chacun plus de 1000 bits. 
    

**Algorithme d’El Gamal:**  
Se base sur le principe de Diffie-Hellman et sur le problème du logarithme discret : c’est facile de calculer y = ax mod n, mais difficile de trouver x

### Signature numérique (digital signature)  
Objectifs d’une signature numérique : authentification, et non répudiation. Ces signatures se basent sur la cryptographie asymétrique, et on utilise la clé privée pour signer un certificat.  

**Il faut d’abord générer un « résumé » du message (« message digest »):** un hash. L’idée est de générer un résumé de longueur fixe (et petite), unique et qui a l’air aléatoire, pour n’importe quelle donnée. Il faut pour cela une fonction (de hash) qui n’est pas inversible, qui prend n’importe quel input (peu importe la longueur), qui renvoie un output de longueur fixe, qui a l’air aléatoire, qu’il ne soit pas facilement possible de trouver 2 données ayant le même hash. Et il faudrait qu’un petit changement de l’input ait un impact sur l’ensemble de l’output (du hash). Exemples :  
•	MD2 : hash de 128 bits, obsolète, on a trouvé des faiblesses à cet algorithme et des collisions. 

•	MD5 : hash de 128 bits, obsolète, plus rapide et plus résistant que MD2, on a trouvé des faiblesses, mais pas de collisions.

•	SHA-1 : pour des messages de longueur inférieure à 2 ⁴ bits. Obsolète, hash de 160 bits, ⁶ possibilités). faiblesses et collisions trouvées. 

•	SHA-2 et SHA-3 : résistants à la cryptanalyse.

**On utilise ensuite le hash du message pour vérifier l’intégrité du message:**  

•	HMAC – Hash Message Authentification Code : supposons que A et B connaissent tous les deux une clé secrète k. A calcule h = H(f(k,p)) (où H est une fonction de hachage et f une fonction qui mélange k et le message p). A envoie ensuite p et h à B. B peut lui aussi calculer H(f(k, p)), et vérifier si le résultat est bien identique au « h » qu’il a reçu.
i.	La fonction f ne doit pas être bêtement « f(k, p) = concat(k, p) », sinon comme f est connue, cela pourrait permettre de trouver k.
ii.	Cette méthode ne règle pas le problème de l’authentification 

•	Signature numérique basée sur la cryptographie asymétrique : A calcule h = H(p), puis utilise sa clé privée pour chiffrer h : signatureA = E(KA - , h), et envoie p et signatureA à B. B peut déchiffrer signatureA à l’aide de la clé publique de A : h’ = D(KA + , signatureA), et calculer h’’ = H(p), et vérifier si h’ == h’’. Si c’est le cas, c’est que le message p n’a pas été modifié, et qu’il a bien été envoyé par A. 
i.	Contrairement à une signature réelle (manuscrite), 2 messages signés avec la même clé auront 2 signatures différentes, et un message signé avec 2 clés différentes aura 2 signatures différentes. Il n’est donc pas possible de copier la signature d’un message pour un autre message.
ii.	DSA – Digital Signature Algorithm : se base sur le problème des logarithmes discrets, utilise SHA-1 comme algorithme de hash. 

**Exemple:**
horodatage certifié (trusted timestamping) : système permettant de garder la preuve de l’existence d’un document et de son contenu à une date donnée, en se basant sur une autorité de certification. Fonctionnement :   
1. Le client calcule le hash de son document (h : H(document), et l’envoie à l’autorité d’horodatage. 

2. L’autorité d’horodatage mélange le hash reçu (h) au timestamp actuel (via une fonction connue), et calcul le hash de ce mélange (h2 = H(h, timestamp)). L’autorité va ensuite chiffrer ce hash à l’aide de sa clé privée : cert = E(Kautorité, h2), et renvoyer « cert » et le timestamp au client. 

3. Si on possède le document, le certificat et le timestamp, on sait vérifier que le certificat est valide (et que le document donné correspond bien au document lié au certificat) : on calcule le hash du document, on le mélange au timestamp, on hash le résultat, et on vérifie que ce hash correspond bien à celui stocké dans le certificat en déchiffrant le certificat avec la clé publique de l’autorité de certification (K+ autorité) 

### Infrastructure de clés publiques   

  Certificats numériques : se basent sur la cryptographie asymétrique et permettent d’établir une relation vérifiable (prouvable) entre une clé publique et l’identité de son propriétaire (une personne ou un système). 

  **Certificat X.509:**  
  Un certificat numérique. Il est fournit par un tiers de confiance : une autorité de certification. Cette autorité signe numériquement le certificat, pour l’authentifier. Le certificat identifie le sujet (une personne/un système) et l’émetteur en utilisant une notation précise. Le certificat possède également une date de début et de fin de validité.  
  Il y a eu plusieurs évolutions des certificats X.509 (v1, v2, v3). Un certificat contient les informations suivantes : la version du certificat, un numéro de série, le nom de l’algorithme de signature, le nom de l’émetteur, la période de validité, le nom du sujet, la clé publique du sujet, l’identifiant unique de l’émetteur, l’identifiant unique du sujet, la signature du certificat, et des extensions (identifiant de la clé de l’autorité, contraintes basiques, contraintes de noms, etc).  
  Pour pouvoir valider la signature d’un certificat, il faut la clé publique de l’émetteur de ce certificat. L’émetteur (une autorité de certification) auto-signe son certificat (il le signe avec sa propre clé privée). Attention qu’un certificat n’authentifie pas son porteur : il faut d’abord vérifier que le sujet connaît la clé privée correspondant à la clé publique qui se trouve dans le certificat (on peut faire cela en envoyant une information chiffrée avec la clé publique du sujet, et en demandant au sujet de renvoyer cette information déchiffrée).  

  La gestion des certificats est réalisée par un ensemble d’entités qui forment **une PKI – Public Key Infrastructure. Une PKI est composée** :  
  * D’une autorité de certification – CA : elle authentifie les sujets, crée, gère et révoque les certificats (CSR – Certificate Signing Request), elle rend sa propre clé publique disponible (via un certificat, signé avec sa propre clé privée). En pratique, elle utilise une chaîne de certificats (un certificat principal auto-signé – certificat racine, et des certificats intermédiaires signés avec la clé privée liée au certificat principal). 
  * D’une autorité d’enregistrement – RA : elle travaille avec l’autorité de certification, elle reçoit et valide les informations des sujets, elle génère des clés pour les utilisateurs lorsque c’est nécessaire, distribue les périphériques de stockage de clés, elle gère et autorise les requêtes de stockage et de récupérations de clés, ainsi que les requêtes de révocations de certificat. Il y a plusieurs RA pour une CA. 
  * D’un annuaire/répertoire de certificats
  * D’un serveur de récupération de clés : il permet d’éviter qu’il y ai trop de création et de distribution de clés et de certificats lorsqu’il y a une perte.  

  **Révocation de certificat:** Cela peut être nécessaire si on perd sa clé privée ou si elle est compromise, s’il y a des données fausses dans le certificat, si l’autorité de certification fait une erreur, si le sujet du certificat n’existe plus (si c’était un système par exemple). L’autorité de certification maintient une: 

  **CRL – Certificate Revocation List**:  
  un fichier qui contient la liste des certificats révoqués que cette autorité avait signé. Cette liste est mise à jour régulièrement (même s’il n’y a pas de changement), pour garantir que l’information est à jour. Mais cela nécessite que le client vérifie lui-même cette liste, via le protocole OCSP (Online Certificate Status Protocol). La CA peut rendre cette liste disponible via différents endroits s’il s’agit d’une grande CA (pour éviter une surcharge).  
  * Il y a aussi moyen que le serveur fasse lui-même la requête vers la CA pour éviter au client de le faire, via l’agrafage OCSP (OCSP Stapling) : le serveur fournit au client, en même temps que la réponse à la requête du client, une réponse OCSP horodatée et signée par la CA (réponse à une requête que le serveur a fait très récemment). 

  Validation d’un certificat lorsque l’on se connecte à un site web : il faut vérifier que le sujet du certificat correspond bien au site auquel on essaie d’accéder, que le certificat n’a pas expiré, qu’il est conforme à l’utilisation (=valider l’identité du sujet), qu’il a bien été signé avec la clé de l’émetteur, et qu’il n’a pas été révoqué. → pour vérifier la signature du certificat, il faut la clé publique de l’émetteur du certificat (soit le serveur renvoie ce certificat, soit le client doit le récupérer lui-même).  

  **3 classes de certificats:**  
  * *classe 1* (simple vérification : email ou nom de domaine),  
  * *classe 2* (vérification à distance de l’identité du sujet, ex : photocopie de la carte d’identité demandé),   
  * *classe 3* (vérification en face à face de l’identité du sujet), certificat qualifié (certificat émis par une autorité qualifiée, c’est-à-dire une autorité qui se confirme aux exigences de régulation).  

  #### ANSSI recommendation  
  * Clé symétrique : 100b (< 2020) 128b (>2020)  
  * Block cipher : 64b (< 2020) 128b (>2020)  
  * Algorithms : DES, 3DES : KO, AES : OK   
  * Factorisation (RSA)
    * Size of n : 2048b (<2020), 4096b (> 2020)
    * Secret exponent : same as n
    * Public exponent e : > 2^16
* Digest: 
    * Digest size : 200b (<2020)
    * SHA-1 : NOK; SHA-256: OK  

#### Application:  
**Découper un secret en plusieurs parties:** si une personne (T) veut partager un secret p entre A, B, C et D pour qu’à 4 ils puissent connaître le secret, mais pas seuls : T génère 3 chaîne aléatoires R1, R2 et R3, calcule U = p XOR R1 XOR R2 XOR R3, puis donne R1 à A, R2 à B, R3 à C et U à D. Si A, B, C et D rassemblent leur partie, ils peuvent reconstruire p. Si une des 4 personnes perd sa partie, ils ne savent plus reconstituer p (sauf si T a gardé p).

  

  


  #### Pouvoir les orchestrer dans un scénario simple  

  #### Cryptographie symétrique  
  **Distribution de clé via un tiers de confiance – algorithme de Needham-Schroeder:**  
  Alice veut communiquer avec Bob, et Trent est un tiers de confiance. KAT est la clé partagée entre Alice et Trent, et KBT la clé partagée entre Bob et Trent. k est la clé permettant à Alice et Bob de communiquer, et r a et rb sont des nombres aléatoires (nonce). E représente la fonction de chiffrement, le premier paramètre étant la clé.  

![](nnedham.png)
a.	Alice envoie « Alice », « Bob » et « ra » à Trent 

b.	Trent renvoie les 4 informations suivantes chiffrées avec KAT : « ra », « Bob », « k » et « E(kBT, (A, k)) ». 
i.	Alice sait que le message vient bien de Trent (car il est chiffré avec KAT). 

c.	Alice déchiffre les 4 informations et envoie « E(kBT, (A, k)) » à Bob. 
i.	Bob sait que le message vient d’Alice et qu’Alice est bien qui elle prétend être.

d.	Bob déchiffre ce message, il renvoie ensuite le message suivant à Alice, chiffré avec k : « rb ». 
i.	Alice sait que le message vient bien de Bob, car il a réussi à déchiffrer le message chiffré avec kBT. 

e.	Alice déchiffre le message, et renvoie le message suivant à Bob, chiffré avec k : « rb - 1 »

  **Distribution de clé via un tiers de confiance – algorithme de Otway-Rees:**  


![](Otway.png)
#### Cryptographie asymétrique
**Exemple – principe de Diffie-Hellman:**  
A et B veulent communiquer ensemble. Il faut d’abord choisir deux nombres : p (un grand nombre premier) et g (un « générateur pour le groupe cyclique fini G = Z/p2 »), et p et g sont publics. Ensuite, A choisit un très grand nombre « a » et calcule « g a mod p », et B choisit un très grand nombre « b » et calcule « g b mod p ». Ensuite, A envoie « g a mod p » à B, qui peut alors calculer k = (ga mod p)b mod p . Et B envoie « g b mod p » à A, qui peut alors calculer k = (gb mod p)a mod p. « k » ( =gab mod p) est uniquement connu de A et de B !  

* Si quelqu’un écoute les communications entre A et B, il connaît alors « g a mod p » et « g b mod p ». pour connaître k = gab mod p, il faut que cette personne connaisse a (ou b) à partir de ga mod p : c’est le problème du logarithme discret, qui n’a pas de solution simple ! 
* Mais cette technique est vulnérable à l’attaque man-in-the-middle (A ne sait pas vérifier l’identité de B et vice-versa)

  
  
  
  
  
## Chapitre 4: Authentification  
  #### Connaitre les différentes méthodes d’authentification, leurs forces et faiblesses, et pouvoir motiver le recours à l’une ou l'autre  
  **Mot de passe basique**  
  Il ne faut pas noter son mot de passe sur un post-it collé sur l’ordinateur (…), et il faut un mot de passe assez long, utilisant assez de caractères différents (minuscules, majuscules, chiffres et symboles spéciaux) et n’étant pas facile à deviner. Il faut changer régulièrement les mots de passe, limiter le nombre d’essais de connexion ratés (éviter le brute force), imposer un délai variable entre les essais de connexion ratés (ex : attendre 1 minute avant de réessayer, puis 2, puis 5 etc). Toujours changer les mots de passe par défaut. Ne pas changer de mot de passe avant de partir en vacances (par ex) : un mot de passe s’oublie si on ne l’utilise pas régulièrement.  
  **Mots de passes à usage unique:** 
  un mot de passe généré lorsque nécessaire et utilisé seulement une fois. Cela rend les attaques sur les mots de passes inutiles, il n’y a pas besoin de se souvenir du mot de passe et on pourrait les échanger en clair. Mais un tel mécanisme nécessite un état synchronisé et un secret partagé entre le client et le serveur.  
  **Certificat:** composé d’un émetteur (issuer), d’un sujet, d’une clé publique et d’une signature. Comment authentifier : en vérifiant que le certificat est valide, et en envoyant une requête avec la cryptographie asymétrique (et vérifier que la réponse est correcte) : tester que la personne possède bien la clé privée liée au certificat.  
  **Biométrie:** utiliser des données physiologiques ou comportementales : empreintes digitales, scan de l’iris ou rétinien, authentification par la voix, manière de taper au clavier… Le système collecte différents modèles, ensuite pour identifier quelqu’un, il faut trouver un modèle qui correspond à la personne, et ensuite pour authentifier une personne identifiée (une personne dont on a enregistré le modèle), il faut vérifier que les nouvelles données correspondent au modèle enregistré.  

  


  #### Comprendre les différents modes de gestion des données relatives à l’identité numérique et en identifier les composants et leur fonction  
  **LDAP – Lightweight Directory Access Protocol:**  défini à la fois un modèle de données et un protocole d’accès aux données. Les données sont organisées en arbre (DIT – Directory Information Tree), elles sont stockées dans des nœuds de l’arbre. La structure des nœuds est définie dans un schéma extensible, et tous les éléments de ce schéma sont identifiés par un identifiant unique. Le nœud racine est identifié par un suffixe (ex : « dc=unamur, dc=be »).  
  **Service d’authentification externe:**  un tel service permet aux applications de ne pas se préoccuper du processus de gestion des identités et du protocole d’authentification. Il faut pour cela un protocole sécurisé, et il faut avoir confiance au service d’authentification. Il y a deux rôles dans un tel système : un fournisseur d’identité (Identity provider) et des fournisseurs de services. Ce fonctionnement permet le « single sign-on » : on se connecte une fois et on a accès à plusieurs services.(OpenId, Oauth, SAML)   

  

## Chapitre 5: Autorisation
### Connaitre les différents modèles de contrôle d’accès, leurs forces et faiblesses, et pouvoir motiver le recours à l’un ou l'autre.

#### DAC : Discretionary Access Model

Ce modèle se base sur la propriété/titularité (ownership) des ressources, on a une matrice de permissions où chaque ligne est un sujet (personne/système), et chaque colonne une ressource. Chaque case de la matrice est un booléen qui dit si un sujet à la permission d’utiliser une ressource.

* On peut avoir des groupes de sujets, et des groupes de ressources.
* On peut utiliser des aptitudes ( "capability" ) qui sont des tokens décrivant la permission accordée à un utilisateur. Ex : token oAuth.
* On peut utiliser une liste de contrôle d’accès (ACL) qui est une liste des permissions liées à un objet. Souvent utilisé dans les OS, serveurs LDAP, bases de données…
* On peut transférer la propriété d’une ressource à quelqu’un d’autre.
* Dans certains environnements on peut déléguer des droits.

Ce mode de gestion des accès se base donc sur le fait que le propriétaire d’un fichier est le sujet qui a créé ce fichier, et que c’est lui qui détermine qui aura accès à son fichier. La gestion des accès est complexe avec ce système.

#### MAC : Mandatory Acces Control

Les permissions sont déﬁnies a priori, à la création ou à la conﬁguration du système. L'utilisateur n'a pas de contrôle sur la politique de contrôle d'accès. Le MAC contrôle le ﬂux d'information, ainsi que la conﬁdentialité et l'intégrité de l'information.

Ce modèle se base sur la sécurité à multiniveaux (MultiLevel Security - MLS). Les sujets et les ressources sont étiquetées d'un niveau de sécurité. 

Le principe « Need-to-know » est appliqué, c'est à dire que des sous-classes peuvent être déclarées au sein d'un niveau. Par exemple, un général militaire de la composante Terrestre ne devrait pas avoir accès aux documents Top Secrets de la composante Marine.

##### Bell - La Padula Model

BELL et LA PADULA ont déﬁni un modèle comme suit :

* S, un ensemble de sujets.
* O, un ensemble d'objets.
* A, un ensemble d'opération d'accès.
* L, un ensemble partiellement ordonné de niveau de sécurité (càd que tout les niveaux ne sont pas comparables).
* fS: S → L, définit le niveau maximal de sécurité d'un sujet.
* fC: S → L, déﬁnit le niveau de sécurité actuelle d'un sujet, tel que celui si n'est pas supérieure au niveau maximal du sujet ( fC(s) ≤ fS(S) ∀s)
* fO: O → L, définit le niveau de sécurité d'un objet.deﬁnes the security level of an object
* ss-property: ∀s ∈ S, o∈ O, a ∈ A: ( fO(o) ≤ fS(s)) ∧ (a of type « read ») ⇔permission(s, o, a)
  Empêche que l'on puisse lire des documents de niveau de sécurité plus élevé que le sien (read up).
* *-property: ∀s ∈ S, o∈ O, a ∈ A: ( fO(o) ≥ fS(s)) ∧ (a of type « write ») ⇔ permission(s, o, a)
  Empêche que l'on puisse écrire des documents dans un niveau plus bas que le sien (write down).

![image-20200613175202430](bell-lapadula.png)

Ce modèle nécessite néanmoins un mécanisme d'exceptions, pour, par exemple, envoyer de l'information aux niveaux plus bas. Les solutions trouvées sont une diminution du niveau de sécurité temporaire pour pouvoir écrire un à niveau inférieure au sien (interdit par *-property). 
Le soucis c'est que si un sujet est dégradé à un niveau inférieur elle est sensé oublié ce qu'elle connait des niveaux inférieurs, ce qui n'est pas vraiment possible en pratique. Une autre approche est de définir un sujet de conﬁance qui peut à l'encontre des règles (ex: l'utilisateur root sur un système)
Ce modèle assure la conﬁdentialité des données, mais pas leur intégrité.
Il est souvent utilisé dans des contextes militaires ou des environnements où la sécurité formalisée fortement est nécessaire.

##### BIBA Model

Ce modèle inverse les caractéristiques de BELL et LA PADULA : l'intégrité est garantie, tandis que la conﬁdentialité pas. L'intégrité est garantie par le principe de « no write up » et de « no read down ».

#### Modèle RBAC
Ce modèle permet de remédier aux limitations de MAC (trop rigide) et DAC (difficile à gérer), et il y a un plus haut niveau d’abstraction (niveau plus proche des concepts business).
**Concepts :**
**Objet**: ressource sujette à un contrôle d’accès.
**Opération**: action sur une ressource.
**Permission:** autorisation d’effectuer une opération sur un objet.
**Rôle:** fonction définie dans le contexte d’une organisation : défini l’autorité et les responsabilités associées.
**Utilisateur :** sujet qui souhaite effectuer une opération sur un objet (une personne, une machine, un processus…).

Les permissions sont dérivées/déduites des rôles assignés aux sujets, ce qui permet d’appliquer différentes polices : « least privilege » (donner le minimum de droits pour que le sujet puisse réaliser ce qu’il doit faire), séparation statique des rôles (pour éviter les conflits d’intérêts, on peut définir qu’un même sujet ne peut pas avoir le rôle de développeur et de testeur pour une même ressource par exemple), cardinalité des rôles (ex : on peut dire que maximum une personne peut avoir le rôle « directeur »).

Quand une session est ouverte (pour un sujet), on active un sous-ensemble de rôles autorisés. On peut également imaginer d’avoir une séparation des rôles dynamiques : lors de l’activation du sous-ensemble, vérifier qu’il n’est pas incohérent/qu’il respecte certaines règles.

Gestion du modèle RBAC – 3 étapes : définition des permissions, assignation des permissions aux rôles, et assignation des rôles aux utilisateurs. Chaque rôle doit avoir un propriétaire qui gère la définition du rôle, les règles d’assignation du rôle et les incidents liés à ce rôle.

#### ABAC – Attributed-Based Access Control : 

Une méthode de contrôle d’accès pour laquelle les requêtes (réalisées par les sujets) d’accès aux ressources sont accordées/refusées en fonction des attributs du sujet, de l’objet, de conditions environnementales et d’un ensemble de polices qui sont spécifiées par rapport à ces attributs et conditions.

#### P-RBAC – Privacy Aware RBAC 

Etend RBAC pour inclure des mécanismes liés à la confidentialité (buts, conditions et obligations…).
#### OrBAC – Organization-based Access Control
Centré sur le concept d’organisation, possède un niveau abstrait et un niveau concret. Les permissions peuvent être positives ou négatives.

#### Comprendre l’intérêt d’une approche décentralisée de l’autorisation et en identifier les composants et leur fonction

 Comme les politiques de contrôle d’accès sont complexes (contenu et structure), les politiques sont mises en application à plusieurs endroits (firewall, applications…) et qu’il y a une pression réglementaire (pouvoir tracer les accès…), il y a des coûts de gestion et de maintenance importants, de gros risques d’avoir les politiques et des décisions incohérentes et c’est difficile de tracer (enregistrer) tout ce qui se passe.
Vers une approche distribuée : il faudrait idéalement une définition et une gestion centralisée des politiques, adopter un modèle et un langage commun pour exprimer les politiques, pouvoir gérer facilement ces politiques (écrire, valider, mettre à jour, approuver, combiner, appliquer…), et séparer le flux de décisions (d’accès) du flux normal des applications : les politiques doivent être définies en dehors de l’application, les décisions prises en dehors de l’application, et l’application doit uniquement exécuter la décision.
Modèle XACML : standard générique et extensible, modèle distribué de contrôle d’accès. Il définit un langage pour les politiques, et un protocole de communication. Ce modèle est composé de nombreux composants : PAP (Policy Administration Point), PEP (Policy Enforcement Point), PDP (Policy Decision Point), PIP (Policy Information Point), et un Context Handler. 

![image-20200613195620596](XACML.png)

Fonctionnement :

1. Une personne envoie une requête pour accéder à une ressource, en contactant un PEP (morceau d’une application).
2. Le PEP envoie la requête au gestionnaire de contexte.
3. Le gestionnaire de contexte construit un contexte de requête XACML et l’envoie au PDP.
4. Le PDP récupère les politiques de l’application depuis le PAP.
5. Le PDP demande des attributs du sujet, des ressources, actions et de l’environnement au gestionnaire de contexte.
6. Le gestionnaire de contexte demande les attributs à un PIP.
7. Le PIP s’occupe de récupérer les différents attributs (de la ressource, de l’environnement et du sujet).
8. Le PIP renvoie les infos au gestionnaire de contexte, qui les renvoie au PDP.
9. Le PDP évalue les politiques, et renvoie une réponse au gestionnaire de contexte.
10. Le gestionnaire de contexte traduit la réponse dans le format de réponse natif du PEP et renvoie
cette réponse au PEP.
11. Le PEP rempli ses obligations.

## Chapitre 6: Sécurité de l'infrastructure
  * **Connaitre les défis pour la sécurité de l’infrastructure et les pistes de solution**  
  Pas la confidentialité ni l’intégrité (qui sont plutôt des problématiques de plus haut niveau), mais plutôt la disponibilité : il faut que l’entreprise puisse continuer à tourner, et comme l’entreprise dépend de ses infrastructures, celles-ci doivent rester disponibles. Il faut éviter des pannes d’infrastructures inattendues : utilise du matériel fiable, maintenable (on peut mettre à jour le firmware facilement, remplacer un composant facilement, etc), utiliser de la détection et correction d’erreurs, et permettre une reconfiguration automatique de l’infrastructure en cas de problème.  
  **Solution?**  
  * Redondance des composants
  * Redondance du système 
  * Robustesse : ne pas rendre les choses trop complexes : garder le système aussi simple que possible. Il faut minimiser la probabilité de défaillance, et garder le système gérable (bien le connaître). 
  * Sécurité réseau, hébergement 
  * Redondance des sites (lieux), monitoring et contrat  
  
  * **Pouvoir proposer une solution graduelle à un risque d’indisponibilité, en veillant à articuler les différentes lignes de défense (prévention, détection, récupération) et les types de contre-mesures (techniques, organisationnelles, juridiques)**
    ### Redondance des composants :  

**Défaillances** : il y en a différents types : des défaillances qui surviennent au début (défauts de fabrication par ex), à la fin de la vie du produit (quand il devient trop vieux, s’use), et les autres (qui peuvent survenir à tout moment). On constate donc plus de défaillance au début et à la fin de la vie des produits, qu’au milieu.

**Redondance CPU** : on peut avoir plusieurs CPU, que l’on peut remplacer pendant que le système tourne.
**Redondance mémoire** : mécanismes de détection et correction d’erreur  
**Redondance I/O** : avoir plusieurs interfaces, permettant une plus grande bande passante → En cas de défaillance, le transfert se fera avec une moins grande bande passante.
Câblage : rester organisé (bonne longueur, les nommer (étiquettes) et bien les placer).
Redondance des sources d’alimentation : avoir des sources alternatives ! Surtout pour gérer des pannes temporaires de courant (et pouvoir éteindre le système correctement en cas de prolongation de panne).  
**Interventions (de réparation) sur le hardware** : délicat ! Il faut faire fort attention, il faut de l’expérience et les bons outils pour éviter de perturber le matériel sensible.
Redondance des supports de stockage de données : RAID – Redundant Array of Inexpensive Disks : on définit des volumes logiques virtuels (répliqués) au-dessus de disques physiques. Cela utilise différents mécanismes : mirorring (une donnée se trouve sur plusieurs disques durs), striping (découper une grosse donnée en plusieurs parties et stocker chaque partie sur un disque différent, pour accélérer la récupération de cette donnée) et contrôle de parité.  
* **RAID 0** : du striping, mais pas de redondance. RAID 0 permet de créer des volumes (virtuels) plus gros que la taille d’un seul disque dur. 
* **RAID 1** : redondance des données ! On a 2 disques qui sont identiques (un des disques est une copie de l’autre).
* **RAID 5** : striping, contrôle de parité : on a n disques durs, et l’équivalent d’un disque des dur contient les bits de parité (ex : on fait un XOR entre le premier bit de chacun des disques durs, et on stocke le résultat dans le dernier disque. Si un des disques tombe en panne, on sait recalculer son contenu).  
* **RAID 6** : identique à RAID 5, mais fonctionne aussi si 2 disques durs tombent en panne.  

**→ Contrôleur RAID** : pour contrôler l’accès aux données (les applications utilisant le disque dur ne savent pas qu’un système RAID est mis en place et ne doivent pas s’en préoccuper). Le contrôleur peut être logiciel ou matériel.   
**Unité de stockage** : système de stockage spécialisé, composé de disques durs mais aussi de CPU et de RAM. Un tel système permet des fonctionnalités plus avancées (ex : faire un snapshot d’un disque : sauvegarder l’état d’un disque dur à un moment donné).

  ### Redondance du système 
  
  **Virtualisation** : concept assez ancien (et toujours très utilisé!) : partager une ressource physique parmi plusieurs clients, en créant des ressources virtuelles au-dessus de cette ressource physique. Ex :
mémoire virtuelle, volumes logiques (stockage de données), VLAN (réseau virtuel), timesharing (partage d’un processeur parmi plusieurs clients)…   
**Clusters de machines** : On peut utiliser des clusters de serveurs et de la virtualisation : le service tourne sur une machine virtuelle liée à un cluster de machines physiques. Si une des machines tombe en panne, les autres peuvent prendre le relai (soit le service était assigné à une machine, et on lui assigne une autre machine, soit il y avait déjà du load balancing et il suffit de rediriger les requêtes vers des machines fonctionnelles). Les machines d’un cluster peuvent avoir un état partagé.  

**Load-balancing** : utile pour les services stateless (les requêtes sont auto-contenues, le serveur n’a pas besoin d’informations supplémentaires provenant de requêtes précédentes pour pouvoir répondre à la requête) tels que des serveurs web, DNS, LDAP… Le principe : on a un ensemble de serveurs, et on répartit la charge (les requêtes) parmi ces différents serveurs. Il faut un algorithme qui choisit vers quel serveur envoyer une requête (soit simplement un serveur à la fois, soit un algorithme plus complexe).
  
  ### Sécurité réseau, hébergement :  
  
  **Disponibilité du réseau** : redondance ! Avoir plusieurs cartes réseau, plusieurs fournisseurs, plusieurs canaux de connexion.  
**Contrôle d’accès** : le réseau donne accès aux serveurs et à l’organisation entière, il faut le protéger : firewall : filtrage de paquets, inspection des paquets (stateful!), firewall applicatifs, contrôle des appareils connecté…   
**Isolation du réseau** : consiste à découper le réseau interne en plusieurs sous-réseaux isolés, et avoir des zones déconnectées d’internet et des accès extérieurs (DMZ – demilitarized zones), des réseaux internes (LAN) et externes (WAN, lié à internet). Il faut gérer les transmissions de données entre le LAN et le WAN, le WAN et le DMZ et entre le LAN et le DMZ, en utilisant différents firewalls.  
**Confidentialité, intégrité** : il faut utiliser des canaux chiffrés (pas forcément toujours sur la même couche réseau) : PGP (emails), DNSSEC, SSL/TLS, IPSec, VPN (tunnel sécurisé entre un ordinateur et le réseau d’une organisation).   
**CASB – Cloud Access Security Broker** : fournis un point de contrôle cohérent et pratique sur les activités et données des utilisateurs au sein d’un réseau complexe (nombreux services et applications basées sur le cloud).   


**Hébergement – datacenter** : service d’hébergement complet, offrant de nombreux services (contrôle d’accès, câblage, bâtiment sur mesure). Il faut bien évidemment que ce datacenter soit correnctement ventilé, correctement fourni en électricité et ayant des procédure de gestion de services à grande échelle

### Redondance des sites (lieux), monitoring et contrat
  **RPO** – Recovery Point Objective : délai entre le moment où on a sauvegardé les données pour la dernière fois, et le moment d’un incident.  
**RTO** – Recovery Time Objective : délai entre le moment où il y a un incident, et celui où le système redevient fonctionnel du point de vue des utilisateurs.

**Récupération après un désastre – approche générale** : définition des objectifs et du périmètre de la procédure, identification des systèmes, définition de paramètres (RTO, RPO), des équipes et des responsabilités de chacun, concevoir un plan haut niveau (les principes), puis un plan plus technique, implémenter la solution, élaborer des procédures, former les gens à appliquer ces procédures, et les tester régulièrement ! Puis évaluer et améliorer l’ensemble des solutions et procédures.   
→ Les sites (lieux) de backup peuvent appartenir à l’entreprise, mais on peut aussi imaginer louer un lieu de backup à un partenaire, ou simplement outsourcer. Où placer le lieu de sauvegarde : pas trop près (pour éviter que les 2 subissent le même incident), et pas trop loin (délais de communication).  

**Type de synchronisation** : soit le site de sauvegarde est une copie exacte du site principal, soit le site de sauvegarde ne peut pas remplacer le site principal mais permet de restaurer le site principal. On peut aussi avoir une approche hybride : en temps normal, on fait fonctionner les 2 sites (principal et secondaire), et si un des 2 tombe en panne, l’autre sait prendre le relai.  
Si le site de backup est éloigné du site principal, il y a différentes techniques de synchronisation : mirorring asynchyrone, envoi de logs (liste des actions effectuées sur une donnée par exemple), synchronisation de fichiers (rsync), etc.  

**Le monitoring (surveillance) et la mise en place d’alertes** sont également importants ! Si quelque chose se passe mal, il faut en être informé rapidement. Il faut réfléchir ce qu’il faut surveiller, comment mettre en place la surveillance sans ralentir le système (en diminuant les performances…), essayer d’éviter les faux négatifs et faux positifs. Alertes : email, sms, équipe de support…   
En cas de sous-traitance pour la redondance des sites, il faut faire attention au fournisseur que l’on choisit : il doit avoir une ligne de produits flexibles, modulaires et à jour, on doit pouvoir aller voir sur place, il doit déjà avoir des partenaires, etc. Et il faut écrire un SLA – Service Level Agreement : un document qui définit la qualité de service du fournisseur (disponibilité, distinguer les défaillances graves et légères, définir les responsabilités, la communication, etc). 




## Chapitre 7: Sécurité du système  
  * **Connaitre les défis pour la sécurité du logiciel système et les pistes de solution**  
  
 ### Missions de l’OS:  
 gérer les ressources (cpu, mémoire, périphériques) en les allouant, partageant et en contrôlant les accès, fournir une couche d’abstraction (interface stable et simple, indépendante du hardware), et c’est la dernière ligne de défense entre un attaquant et un système.
 * *Processus*: partie dynamique du système, il exécute du code qu’il lit depuis un fichier, accède à des ressources (cpu, mémoire, fichiers…), et s’exécute au nom d’un utilisateur (il hérite des privilèges de cet utilisateur).   
 * *Utilisateur*: possède un compte sur la machine, appartient à des groupes, possède un ensemble de permissions. Il y a des utilisateurs normaux, des admins, et des comptes spéciaux.  
 * *Organisation du stockage*: : mémoire non volatile, organisation hiérarchique (fichiers et dossiers), les fichiers sont organisés en un système de fichiers (fat, ntfs, ext4…) qui ajoute des fonctionnalités (liens, chiffrement, journalisation), définit des métadonnées (nom de fichier, date de création/modification, permissions, checksum…), définit l’organisation de l’espace disponible (inodes), définit des limites (taille maximale d’un fichier, taille d’un volume, contraintes sur le nom des fichiers…). Cette organisation peut avoir un gros impact sur la sécurité d’un système.
  
  
  * **Pouvoir proposer une solution complète à ces défis, en veillant à articuler les différentes lignes de défense (prévention, détection, récupération) et les types de contre-mesures (techniques, organisationnelles, juridiques)**  
  
  ### Menaces et contre-mesures  
  Il y a beaucoup de menaces potentielles : bombe logique (morceau de code malicieux dans un système qui se déclenche à un moment donné ou si une condition est respectée), backdoor (point d’entrée caché vers un système), usurpation de connexion, augmentation/escalade des privilèges…  
  **Login spoofing** : fausse interface de login  
  **Backdoor** : faire un accès avec un identifiant reconnu  
  **Excalation de privilège** : mécanisme pour avoir plus de privilège que l’on a accès  
  **Logic Bomb** : programme malicieux qui démarre à un moment donné (par exemple un employé viré)  
  
  **Attaque DoS – Denial of Service** : consister à « innonder » un système de requêtes :  
  * **UDP Flood** : envoyer plein de datagrammes UDP vers la victime, sur différents ports. Et on met une fausse adresse ip source pour éviter de se faire détecter. 
  * **ICMP Flood** : idem UDP Flood, mais avec des requêtes ICMP 
  * **TCP SYN Flood** : on commence plein initialisations TCP (handshake) sans la terminer, ce qui fait que le système victime va réserver plein de ressources pour ces requêtes. 
  * **TCP RST** : envoyer des requêtes RST pour fermer des connexions ouvertes (mais pour cela, il faut connaître les numéros de séquence). 
  * **Sockstress** : envoyer une requête indiquant que la « window size » vaut 0 (ce qui va bloquer les transmissions futures jusqu’à ce que cette taille change à nouveau). 
  * HTTP flood, DNS flood…   
  * **Attaque DoS par réflexion** : type d’attaque DoS, où on utilise un système tiers pour attaquer, pour masquer l’identité de l’attaquant. 
  * **Attaque DoS amplifiée**: faire en sorte que le volume de données reçues par la victime soit plus important que le volume de données envoyé par l’attaquant. Ex :
    * DNS Amplification attack : envoyer des requêtes DNS à des serveurs DNS qui renvoient de grosses réponses, en utilisant l’adresse ip de la victime dans ces requêtes (pour que ce soit la victime qui reçoive les réponses).   
    * Smurf Attack : envoyer des requêtes ICMP vers des adresses broadcast, en utilisant l’adresse ip de la victime.  
    
**Malware** : terme générique qui couvre un grand nombre de menaces différentes. Aujourd’hui, les attaques sont de plus en plus complexes, et peuvent être développées par des organisations criminelles, des gouvernements, des entreprises, etc. Effets des attaques : chantage, relai de spams, attaques ddos via les infrastructures de la victime, vol de données et d’identité, stockage et partage de fichiers illégaux, propagation de logiciels malveillants, faire transiter (via les infrastructures de la victime) des communications malveillantes, manipulation de sondages, craquage de mots de passes…  
→ Pertes de données, indisponibilité du système, pertes financières… et cela prend du temps de réparer le système.  

Un malware se propage en exploitant des vulnérabilités. « Exploit zero-day » : utiliser une vulnérabilité qui n’as pas encore été corrigée (et qui n’est pas forcément connue du public/du propriétaire du logiciel). → Il y a un marché des vulnérabilités (illégal : achat d’exploit zero-day ; légal : les entreprises offrent souvent des récompenses aux personnes qui signalent des failles de sécurité).

* **Trojan – cheval de Troie** : un programme contenant des fonctionnalités cachées (généralement malveillantes). Il se propage en se téléchargeant et copiant sur d’autres ordinateurs. Il est exécuté par l’utilisateur, qui pense qu’il s’agit d’un programme normal.
* **Virus** : programme qui se réplique en insérant des copies de lui-même dans d’autres programmes. Il doit être exécuté une première fois. 2 phases : insertion, et exécution. Un virus peut cibler différents composants : secteur boot du disque dur, exécutables binaires… Un virus peut être résident (il est présent dans la mémoire RAM de l’ordinateur, et reste actif en permanence) ou non résident (il s’exécute uniquement quand d’autres programmes se lancent). Les virus sont chiffrés, et modifient leur propre code, pour essayer de ne pas être détectés par les antivirus (→ il faut donc une fonction de déchiffrement et décodage).
* **Ver** : programme qui se propage via le réseau. Ex : « Internet Worm », un ver de 1988 qui se propageait de 3 manières différentes (exploitation d’un buffer overflow, bug de sendmail…).
* **Logiciel espion (spyware)** : logiciel caché, qui collecte des données (surveillances, données utilisées pour faire du marketing, etc), les envoie vers un serveur externe, et résiste aux tentatives de suppression. Il se propage via des téléchargements infectés ou des barres d’outils de navigateur infectées. Il affiche par exemple des pubs, fait apparaître des fenêtres popup, modifie ce qu’affiche le navigateur, etc.
* **Rootkit** : programmes et fichiers cachés dans les basses couches du système (OS, et plus bas que l’OS), très résistants à la détection, qui modifient le système en profondeur. Le but est de surveiller/contrôler une machine. Ces programmes ont différentes cibles : bibliothèque (il se cache dans une bibliothèque, ex : libc), noyau (se fait passer pour un composant de l’OS), ou hyperviseur (le programme modifie la séquence de boot pour installer un hyperviseur, qui va lancer ensuite l’OS).
    * Méthodes de détection de rootkit : démarrer le système depuis un disque externe, surveiller le comportement du système et les performances, surveiller certains logs spécifiques…
    
### Comment se protéger des malwares : 
équivalent du jeu du chat et de la souris… On crée des systèmes de détection, et de nouveaux malwares apparaissent, qui arrivent à contourner les systèmes de détection.
1.	Prévention : sélectionner un OS sécurisé et le rendre plus sécurisé (bien le configurer), vérifier les origines et l’intégrité des logiciels et données, filtrer les pièces jointes d’emails, bloquer les périphériques amovibles (usb, cd…), appliquer une bonne isolation du système et des processus, utiliser du contrôle d’accès et des privilèges… -> FORMATION
2.	Détection : utiliser un logiciel anti-malware réputé, et le maintenir à jour, surveiller le système pour détecter des changements inattendus de comportement.
3.	Récupérer : faire des backup régulièrement (avoir plusieurs versions, et plusieurs copies/supports différents).

### Gestion de l’OS et Backup

**Quels mécanismes de protection** mettre en place au niveau de l’OS : mécanismes d’authentification, contrôle d’accès aux ressources de l’OS, fonctions cryptographiques, audit des activités du système (audit trail), firewall, système de détection d’intrusions (IDS), anti-malware, scanners de vulnérabilités.

#### Gestion des ressources :  
* **Principe d’isolation** : gestion et protection plus simple des ressources, possibilité de configurer les privilèges de manière très précise, limiter l’impact d’un problème potentiel à une seule partie du système. Utiliser des environnements différents pour le développement, le test, la production. Et définir des procédures de migration du système.  Ex : chroot, sandbox, docker, virtualisation…
* **Gestion des utilisateurs** : c’est la base du contrôle d’accès et de la surveillance/audit. Il faut suivre le principe du minimum de privilèges (ne pas en donner plus que nécessaire) : environnement sandbox, restrictions d’accès (heure, origine de la requête), mots de passe, date de validité d’un compte… Éviter les comptes partagés (plusieurs personnes qui accèdent au même compte), plutôt utiliser l’augmentation/délégation de privilèges (ex : sudo).
* **Gestion des ressources** : contrôler la quantité de ressources utilisées par utilisateur/par processus (CPU, espace mémoire RAM, espace disque, bande passante…).
* **Gestion du stockage** : sélectionner un système de fichier approprié, avec des volumes de la bonne taille (éviter de risquer d’avoir un volume plein), contrôle d’accès (fichier : exécutable?)
* **Sécurité du réseau** : firewall local, le configurer avec des règles sur mesure, qui dépendent des exigences réelles. Et favoriser les connexions chiffrées (ex : ssh). 

#### Gestion de l’OS :

* **Installation et configuration – configuration initiale** : partitionnement du disque, configuration du réseau, mots de passes par défaut et politiques, installer uniquement ce qui est nécessaire (pour minimiser la maintenance et les risques)
* **Maintenance** : patch, mises à jour, mises à jour majeures (de l’OS entier).
* **Documentation** : pour préserver la connaissance, pouvoir reconstruire le système facilement en cas de soucis. → Formation !  

**Patch et mises à jour:** pour corriger les vulnérabilités au plus vite. Il faut tester les patch avant de les appliquer (pour être sûr qu’ils fonctionnent bien correctement), définir une procédure de patch (manuelle ou automatique?). Pour les mises à jour majeures de l’OS : automatiques, ou on installe manuellement la nouvelle version ?   

**OS Hardening (renforcement)** : limiter les attaques possibles en limitant la quantité de vulnérabilités : fermer les portes non utiles (ports réseau par ex), supprimer/désactiver les services et applications non utilisés, configurer le firewall en fonction de l’utilisation de la machine, installer des anti-malware et lancer une analyse complète du système, vérifier la liste des utilisateurs, vérifier les permissions des fichiers, utiliser des canaux de communication chiffrés… 

#### Définir un plan de backup : 
* **quoi** (que faut-il sauvegarder ? système, données, logiciels, logs…)
* **comment** (dump du système de fichiers et des bases de données, synchronisation de fichiers, outil dédié…)
* **quand** (fréquence, combien de temps garder les données → limiter l’impact sur le système), infrastructure spécifique (stockage, réseau). 
* **Règle 3-2-1** : avoir au total 3 copies des données, 2 sont locales mais sur des médias différents (périphérique différent), et au moins une copie est sur un autre site (assez éloigné).

**Sécurité des backups** : protéger l’accès aux médias et appareils, stocker de manière sécurisée, chiffrer les données, LOCKSS : lots of copies keep stuff sage, vérifier que les backups fonctionnent bien régulièrement.

### Monitoring/surveillance   
La détection d’attaques et de malware est très importante, et donc la surveillance du système aussi (pour pouvoir répondre rapidement). On peut surveiller la disponibilité, les performances… Il existe de nombreux outils (surveillance locale : audit du système ; gestion du réseau et des configurations ; produits opensource ou commerciaux). Paramétrer ces outils peut prendre beaucoup de temps.  

**Gestion des événements:** pouvoir enregistrer (log) l’ensemble des événements du système (et donc des différents services qui tournent) → collecter, stocker, analyser et corréler les événements pertinents provenant de tous les composants de l’infrastructure.  
**Systèmes de détection d’intrusions (IDS – Intrusion Detection System)** : détecter les événements qui pourraient révéler une intrusion (la prévention n’est pas suffisante, il faut aussi pouvoir détecter !). 3 fonctions pour ces systèmes : récolter et enregistrer les données (capteurs distribués partout, sondes placées aux bons endroits), analyser les données (normaliser les données et les analyser), et détecter les intrusions (et alerter). Deux approches :  
* **Détection d’abus** : basé sur la connaissance et la détection de signatures : on se base sur les caractéristiques des attaques (activité du système, trafic réseau, messages de log), connaissant les caractéristiques de nombreuses attaques différentes (base de données de signatures, approches les plus courantes…)
* **Détection d’anomalies** : basé sur le comportement, s’appuie sur une base de référence : une alerte est générée dans le cas où le comportement mesuré du système s’éloigne trop du comportement habituel (comportement de référence). Cette méthode est capable de détecter de nouvelles attaques. Mais de nouveaux comportements peuvent être détectés (à tort) comme étant des attaques. Ce n’est pas facile de définir correctement un comportement « normal ». 

**IDS** : host-based (se base sur des informations locales au système) ou network-based (se base sur l’analyse du trafic réseau). 
**→ Fiabilité (des IDS)** : les faux positifs peuvent être coûteux (en temps) et impactent la confiance que l’on a du système. Les faux négatifs sont de réels problèmes également (on ne détecte pas un problème).  
→ Il faut donc bien configurer ces outils pour avoir le minimum de faux positifs et de faux négatifs.  

**→ Limitations (des IDS)** : les attaques sont souvent ciblées à un système spécifique (et donc adaptées à ce système), les bases de données de signatures évoluent sans cesse, le trafic est souvent chiffré ou fragmenté (plus difficile à analyser), et il y a souvent beaucoup à analyser (ce qui peut diminuer les performances du système)

**SNORT** : un exemple d’IDS, basé sur les signatures. OSSEC : un exemple d’IDS, host-based. Il peut par exemple automatiquement adapter les règles d’un firewall, désactiver des comptes utilisateurs, mettre des fichiers en quarantaine. Il est extensible et adaptable, possède une interface web, peut envoyer des alertes (email, sms), et possède de nombreuses fonctionnalités.
**Contrôle/vérification de vulnérabilités** : mesure proactive (vérifier s’il y a des vulnérabilités). Il existe de nombreux outils automatisés permettant d’identifier et rapporter des vulnérabilités. Ils vérifient beaucoup de choses : découverte d’atouts, scan de vulnérabilités, mauvaises configurations, failles d’applications, malware, identification d’informations sensibles… 
* Outils : nmap (découverte réseau, audit de sécurité, administration), nessus (scan d’OS, d’appareils sur le réseau, firewall, hyperviseur, bases de données, serveurs web), openVAS (scanner de vulnérabilités open source), metasploit (framework de pentest complet), LSAT (linux security audting tool)…


## Chapitre 8: Sécurité logicielle
#### Connaitre les différentes étapes du cycle de vie du développement logiciel et les mesures applicables à chacune d’entre elles.



Pouvoir compléter un cas d’utilisation (use case) par les comportements malicieux et de remédiation selon l’approche des misuse cases

#### Connaitre les grands types de vulnérabilités logicielles et les pistes de remédiation

##### Buffer Overflow : 

Lorsque l’on reçoit un input d’un utilisateur, on peut par exemple le stocker dans un tableau (une variable). Mais si la taille du tableau est fixe (c’est le cas en C par exemple), que l’utilisateur envoie trop de données par rapport à la taille du tableau, et que l’on ne vérifie pas qu’il n’y a pas trop de données (certaines fonctions d’input en C ne vérifient pas cela), un buffer overflow est possible. Par exemple : on crée un tableau de 20 caractères, puis on appelle la fonction C « gets », avec en paramètre l’adresse du tableau (c’est-à-dire l’adresse du premier élément!). gets récupère des caractères entrés par l’utilisateur et les stocke dans la mémoire, à partir de l’adresse
reçue. gets ne connaît donc pas la taille du tableau, et si trop de caractères sont entrés par l’utilisateur, gets continuera d’écrire dans la mémoire (en écrasant d’autres variables, par exemple!).

* Si juste à côté du tableau, dans la mémoire, il y a une variable qui contient les permissions de l’utilisateur, un buffer overflow pourrait permettre à l’utilisateur de modifier cette variable et donc ses permissions au sein du programme !
* Si juste à côté du tableau, il y avait du code, c’est très risqué (on pourrait remplacer une instruction par une autre)… Mais généralement le code et les données ne sont pas complètement mélangés dans la mémoire RAM.
* Si juste à côté du tableau, il y avait une adresse mémoire (ex : l’adresse de la prochaine instruction à exécuter, ou l’adresse de retour d’une fonction), on peut imaginer la modifier (via le buffer overflow) pour exécuter un autre code (qui peut être malveillant).
  * Pour tout ça, il faut essayer de deviner ce qui est stocké juste à côté du tableau dans la mémoire, ce qui n’est pas toujours évident. Et si on veut modifier une adresse mémoire par une autre, il faut pouvoir connaître la bonne adresse (surtout si elle est relative).

##### Bug de formatage de chaînes de caractères

Dans des langages comme le C, le formatage de chaînes de caractères est utilisé dans de nombreuses fonctions (ex : printf("%0.2f", value); → le premier paramètre décrit le format, et les paramètres suivants sont des variables contenant les valeurs à utiliser).
Ces fonctions peuvent poser problème lorsque l’utilisateur peut manipuler le premier caractère : que se passe-t-il s’il la chaîne est vide ? Que se passe-t-il s’il n’y a pas de variable en 2ᵉ paramètre mais qu’on en utilise quand même une dans le format ? Ex : la chaîne « %n » doit être liée à un pointeur vers un entier, et le nombre de caractères imprimés sera écrit à cette adresse-là.

* Risques : révélation d’informations confidentielles (stockées dans la RAM), modification du flux d’exécution (modifier l’adresse de retour RET) et injection de code !
  * Pour éviter une modification de l’adresse de retour : on ajoute un marqueur (ensemble de bits) entre les variables et RET (dans la mémoire), marqueur contenant une valeur aléatoire que l’on enregistre ailleurs également. Et avant d’utiliser RET, on vérifie si le marqueur a été modifié. Si oui, RET l’a été aussi, et on bloque l’exécution.
  * Au lieu d’injecter du code, on peut également utiliser du code existant (de librairies par exemple).
    → Protection de la mémoire : rendre le stack et le heap non exécutables.
    → Pour éviter les buffer overflow, on peut également utiliser la technique ASLR – Address Space Layout Randomization : l’idée est de  placer les bibliothèques et autres éléments (ex : le stack) à des adresses qui changent aléatoirement entre chaque exécution du programme. Cette technique est implémentée dans les OS modernes. Ne s’applique pas pour les exécutables compilés.



#### Pouvoir identifier et décrire de manière technique et détaillée les vulnérabilités logicielles qui ont été présentées ainsi que les contre-mesures appropriées